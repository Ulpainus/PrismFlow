# æ–‡ä»¶å: run_v2v_v2_with_lora.py
# ç‰ˆæœ¬: v2 - åœ¨v1åŸºç¡€ä¸Šé›†æˆLoRAåŠŸèƒ½ï¼Œç°å·²é›†æˆoptical flowç¨³å®šåŒ–
# æè¿°: ä¿æŒå‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œæ‰©å±•process_video_entrypointå‡½æ•°æ”¯æŒLoRAå’Œoptical flow

import os
import torch
import cv2
from PIL import Image
from tqdm import tqdm
import numpy as np
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from controlnet_aux import LineartAnimeDetector
import shutil
import time
import gradio as gr

# ä»æˆ‘ä»¬åˆ›å»ºçš„åº“ä¸­å¯¼å…¥CycleGANå¤„ç†å™¨
from cyclegan_lib.cyclegan_processor import CycleGANProcessor

# ã€æ–°å¢ã€‘å¯¼å…¥optical flowå·¥å…·
import sys
sys.path.append('optical_flow/scripts')
try:
    from core.local_flow_utils import RAFT_estimate_flow, compute_diff_map, RAFT_clear_memory
    OPTICAL_FLOW_AVAILABLE = True
    print("âœ… Optical Flowæ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ Optical Flowæ¨¡å—åŠ è½½å¤±è´¥: {e}")
    print("âš ï¸ å°†ä½¿ç”¨ä¼ ç»Ÿå¤„ç†æ–¹å¼")
    OPTICAL_FLOW_AVAILABLE = False

# å¯¼å…¥è§†é¢‘å·¥å…·æ¨¡å—
try:
    from video_utils import create_browser_compatible_video, check_video_compatibility
    VIDEO_UTILS_AVAILABLE = True
    print("âœ… è§†é¢‘å·¥å…·æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è§†é¢‘å·¥å…·æ¨¡å—åŠ è½½å¤±è´¥: {e}")
    VIDEO_UTILS_AVAILABLE = False

def setup_directories(output_folder):
    """åˆ›å»ºå¹¶æ¸…ç†å·¥ä½œç›®å½•"""
    input_frames_folder = os.path.join(output_folder, "input_frames")
    output_frames_folder = os.path.join(output_folder, "output_frames")
    if os.path.exists(output_folder):
        shutil.rmtree(output_folder)
    os.makedirs(input_frames_folder, exist_ok=True)
    os.makedirs(output_frames_folder, exist_ok=True)
    return input_frames_folder, output_frames_folder

def extract_frames(video_path, output_folder):
    """å°†è§†é¢‘æ‹†åˆ†ä¸ºå¸§å›¾ç‰‡"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = 0
    frames_list = []  # ã€æ–°å¢ã€‘ä¿å­˜å¸§åˆ—è¡¨ç”¨äºoptical flow
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        # ã€æ–°å¢ã€‘è½¬æ¢BGRåˆ°RGBå¹¶ä¿å­˜åˆ°åˆ—è¡¨
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames_list.append(frame_rgb)
        cv2.imwrite(os.path.join(output_folder, f"{frame_count:05d}.png"), frame)
        frame_count += 1
    cap.release()
    return fps, frame_count, frames_list  # ã€æ–°å¢ã€‘è¿”å›å¸§åˆ—è¡¨

def create_video(frames_folder, output_path, fps):
    """å°†æ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡å¸§åˆæˆä¸ºè§†é¢‘ - ä½¿ç”¨æµè§ˆå™¨å…¼å®¹çš„ç¼–ç æ ¼å¼"""
    # å¦‚æœè§†é¢‘å·¥å…·æ¨¡å—å¯ç”¨ï¼Œä½¿ç”¨æ–°çš„å…¼å®¹æ€§å‡½æ•°
    if VIDEO_UTILS_AVAILABLE:
        result = create_browser_compatible_video(frames_folder, output_path, fps)
        if result:
            # æ£€æŸ¥ç”Ÿæˆçš„è§†é¢‘å…¼å®¹æ€§
            is_compatible, message = check_video_compatibility(result)
            if is_compatible:
                print(f"âœ… è§†é¢‘å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡: {message}")
            else:
                print(f"âš ï¸ è§†é¢‘å…¼å®¹æ€§è­¦å‘Š: {message}")
            return result
        else:
            print("âŒ æµè§ˆå™¨å…¼å®¹è§†é¢‘åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•")
    
    # å›é€€åˆ°åŸå§‹æ–¹æ³•
    frames = sorted([f for f in os.listdir(frames_folder) if f.endswith('.png')])
    if not frames:
        return None
    
    first_frame_path = os.path.join(frames_folder, frames[0])
    first_frame_img = cv2.imread(first_frame_path)
    height, width, _ = first_frame_img.shape

    # å°è¯•ä½¿ç”¨H.264ç¼–ç å™¨ï¼ˆæµè§ˆå™¨å…¼å®¹ï¼‰
    try:
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨H.264ç¼–ç å™¨
        fourcc = cv2.VideoWriter_fourcc(*'H264')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            raise Exception("H264ç¼–ç å™¨ä¸å¯ç”¨")
            
        print(f"ğŸ¬ ä½¿ç”¨H.264ç¼–ç å™¨åˆ›å»ºè§†é¢‘: {width}x{height} @ {fps}fps")
        
    except Exception as e:
        print(f"âš ï¸ H.264ç¼–ç å™¨ä¸å¯ç”¨: {e}")
        try:
            # æ–¹æ³•2: å°è¯•ä½¿ç”¨XVIDç¼–ç å™¨
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            output_path_avi = output_path.replace('.mp4', '.avi')
            out = cv2.VideoWriter(output_path_avi, fourcc, fps, (width, height))
            
            if not out.isOpened():
                raise Exception("XVIDç¼–ç å™¨ä¸å¯ç”¨")
                
            print(f"ğŸ¬ ä½¿ç”¨XVIDç¼–ç å™¨åˆ›å»ºAVIè§†é¢‘: {width}x{height} @ {fps}fps")
            
        except Exception as e2:
            print(f"âš ï¸ XVIDç¼–ç å™¨ä¹Ÿä¸å¯ç”¨: {e2}")
            # æ–¹æ³•3: å›é€€åˆ°åŸå§‹mp4vç¼–ç å™¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            print(f"ğŸ¬ ä½¿ç”¨mp4vç¼–ç å™¨åˆ›å»ºè§†é¢‘: {width}x{height} @ {fps}fps")

    # å†™å…¥å¸§
    for frame_name in frames:
        frame_path = os.path.join(frames_folder, frame_name)
        img = cv2.imread(frame_path)
        if img is not None:
            out.write(img)
    
    out.release()
    
    # å¦‚æœä½¿ç”¨äº†AVIæ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºMP4
    if output_path.endswith('.avi') and os.path.exists(output_path_avi):
        try:
            # ä½¿ç”¨ffmpegè½¬æ¢ä¸ºMP4
            import subprocess
            cmd = [
                'ffmpeg', '-i', output_path_avi, 
                '-c:v', 'libx264', '-preset', 'fast', '-crf', '23',
                '-y', output_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… æˆåŠŸè½¬æ¢ä¸ºMP4: {output_path}")
                # åˆ é™¤ä¸´æ—¶AVIæ–‡ä»¶
                os.remove(output_path_avi)
            else:
                print(f"âš ï¸ ffmpegè½¬æ¢å¤±è´¥: {result.stderr}")
                # ä¿ç•™AVIæ–‡ä»¶ä½œä¸ºå¤‡é€‰
                output_path = output_path_avi
                
        except Exception as e:
            print(f"âš ï¸ ffmpegä¸å¯ç”¨: {e}")
            output_path = output_path_avi
    
    return output_path

# --- ã€æ–°å¢ã€‘LoRAåŠ è½½å’Œç®¡ç†å‡½æ•° ---
def load_lora_into_pipeline(pipe, lora_model_name, lora_weight):
    """
    å®‰å…¨åœ°å°†LoRAæ¨¡å‹åŠ è½½åˆ°Stable Diffusion pipelineä¸­
    """
    try:
        if lora_model_name and lora_model_name != "æ—  (None)":
            lora_path = os.path.join("models/Lora", lora_model_name)
            
            if os.path.exists(lora_path):
                print(f"ğŸ¨ æ­£åœ¨åŠ è½½LoRA: {lora_model_name} (æƒé‡: {lora_weight})")
                
                # ä½¿ç”¨diffusersçš„LoRAåŠ è½½æ–¹æ³•
                pipe.load_lora_weights(lora_path)
                
                # è®¾ç½®LoRAæƒé‡
                if hasattr(pipe, 'set_lora_scale'):
                    pipe.set_lora_scale(lora_weight)
                
                print(f"âœ… LoRAåŠ è½½æˆåŠŸ: {lora_model_name}")
                return True
            else:
                print(f"âš ï¸ LoRAæ–‡ä»¶æœªæ‰¾åˆ°: {lora_path}")
                return False
        else:
            print("â„¹ï¸ æœªé€‰æ‹©LoRAæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            return True
            
    except Exception as e:
        print(f"âŒ LoRAåŠ è½½å¤±è´¥: {e}")
        print("âš ï¸ å°†ç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹")
        return False

def unload_lora_from_pipeline(pipe):
    """
    ä»pipelineä¸­å¸è½½LoRAæ¨¡å‹
    """
    try:
        if hasattr(pipe, 'unload_lora_weights'):
            pipe.unload_lora_weights()
            print("ğŸ”„ LoRAå·²å¸è½½")
    except Exception as e:
        print(f"âš ï¸ LoRAå¸è½½æ—¶å‡ºé”™: {e}")

# ã€æ–°å¢ã€‘optical flowå¤„ç†å‡½æ•°
def process_frame_with_optical_flow(
    curr_frame, prev_frame, prev_frame_styled, 
    pipe, preprocessor, prompt, config, 
    device, generator
):
    """
    ä½¿ç”¨optical flowå¤„ç†å•å¸§
    """
    if not OPTICAL_FLOW_AVAILABLE or prev_frame_styled is None:
        # å¦‚æœoptical flowä¸å¯ç”¨æˆ–æ˜¯ç¬¬ä¸€å¸§ï¼Œä½¿ç”¨å¸¸è§„å¤„ç†
        curr_frame_pil = Image.fromarray(curr_frame)
        processed_init_image = curr_frame_pil.resize((config["width"], config["height"]))
        control_image = preprocessor(processed_init_image)
        return pipe(
            prompt=prompt,
            negative_prompt=config["negative_prompt"],
            image=processed_init_image,
            control_image=control_image,
            num_inference_steps=config["steps"],
            strength=config["strength"],
            guidance_scale=config["cfg_scale"],
            generator=generator
        ).images[0]
    
    try:
        # ä¼°è®¡optical flow
        next_flow, prev_flow, occlusion_mask = RAFT_estimate_flow(
            prev_frame, curr_frame, device=device
        )
        
        if next_flow is not None:
            # åˆ›å»ºoptical flowå‚æ•°å­—å…¸
            flow_args = {
                'occlusion_mask_flow_multiplier': 5.0,  # å…‰æµé®ç½©å€æ•°
                'occlusion_mask_difo_multiplier': 2.0,   # åŸå§‹å·®å¼‚å€æ•°  
                'occlusion_mask_difs_multiplier': 0.0,   # é£æ ¼åŒ–å·®å¼‚å€æ•°
                'occlusion_mask_blur': 3.0               # é®ç½©æ¨¡ç³Šå¼ºåº¦
            }
            
            alpha_mask, warped_styled_frame = compute_diff_map(
                next_flow, prev_flow, prev_frame, curr_frame, 
                prev_frame_styled, flow_args
            )
            
            # ä¿®å¤æ‰­æ›²çš„é£æ ¼åŒ–å¸§
            warped_styled_frame = (curr_frame.astype(float) * alpha_mask + 
                                 warped_styled_frame.astype(float) * (1 - alpha_mask))
            
            # ä½¿ç”¨æ‰­æ›²å¸§ä½œä¸ºåˆå§‹å›¾åƒ
            init_image = Image.fromarray(warped_styled_frame.astype(np.uint8))
            mask_coverage = np.mean(alpha_mask)
            print(f"ğŸŒŠ ä½¿ç”¨optical flowå¤„ç†ï¼Œé®ç½©è¦†ç›–ç‡: {mask_coverage:.3f}")
            
            # æ ¹æ®é®ç½©è¦†ç›–ç‡é€‰æ‹©å¤„ç†æ¨¡å¼
            if mask_coverage > 0.1:  # å¦‚æœæœ‰è¶³å¤Ÿçš„å˜åŒ–åŒºåŸŸï¼Œä½¿ç”¨inpainting
                mask_image = Image.fromarray(np.clip(alpha_mask * 255, 0, 255).astype(np.uint8))
                processed_init_image = init_image.resize((config["width"], config["height"]))
                mask_resized = mask_image.resize((config["width"], config["height"]))
                control_image = preprocessor(processed_init_image)
                
                return pipe(
                    prompt=prompt,
                    negative_prompt=config["negative_prompt"],
                    image=processed_init_image,
                    mask_image=mask_resized,
                    control_image=control_image,
                    num_inference_steps=config["steps"],
                    strength=0.85,  # inpaintingä½¿ç”¨è¾ƒé«˜å¼ºåº¦
                    guidance_scale=config["cfg_scale"],
                    generator=generator
                ).images[0]
            else:
                # å˜åŒ–è¾ƒå°ï¼Œä½¿ç”¨å¸¸è§„img2img
                processed_init_image = init_image.resize((config["width"], config["height"]))
                control_image = preprocessor(processed_init_image)
                return pipe(
                    prompt=prompt,
                    negative_prompt=config["negative_prompt"],
                    image=processed_init_image,
                    control_image=control_image,
                    num_inference_steps=config["steps"],
                    strength=config["strength"],
                    guidance_scale=config["cfg_scale"],
                    generator=generator
                ).images[0]
        else:
            print("âš ï¸ Optical flowä¼°è®¡å¤±è´¥ï¼Œä½¿ç”¨å¸¸è§„å¤„ç†")
            
    except Exception as e:
        print(f"âš ï¸ Optical flowå¤„ç†å‡ºé”™: {e}")
    
    # å›é€€åˆ°å¸¸è§„å¤„ç†
    curr_frame_pil = Image.fromarray(curr_frame)
    processed_init_image = curr_frame_pil.resize((config["width"], config["height"]))
    control_image = preprocessor(processed_init_image)
    return pipe(
        prompt=prompt,
        negative_prompt=config["negative_prompt"],
        image=processed_init_image,
        control_image=control_image,
        num_inference_steps=config["steps"],
        strength=config["strength"],
        guidance_scale=config["cfg_scale"],
        generator=generator
    ).images[0]

def process_video_entrypoint(
    input_video_path,
    prompt,
    strength,
    seed,
    processing_mode,
    lora_model_name,  
    lora_weight,      
    progress=gr.Progress(track_tqdm=True)
):
    """
    v2ç‰ˆæœ¬ï¼šæ¥æ”¶UIå‚æ•°å¹¶æ‰§è¡Œå®Œæ•´çš„è§†é¢‘å¤„ç†æµç¨‹ï¼Œæ”¯æŒLoRAåŠŸèƒ½å’Œoptical flowç¨³å®šåŒ–ã€‚
    
    å‚æ•°è¯´æ˜:
    - input_video_path (str): è¾“å…¥è§†é¢‘è·¯å¾„
    - prompt (str): æç¤ºè¯
    - strength (float): é£æ ¼åŒ–å¼ºåº¦
    - seed (int): éšæœºç§å­
    - processing_mode (str): å¤„ç†æ¨¡å¼ï¼Œå¯é€‰å€¼ä¸º "Stable Diffusion Only", "CycleGAN Only", "CycleGAN + Stable Diffusion"
    - lora_model_name (str): LoRAæ¨¡å‹æ–‡ä»¶åï¼Œ"æ—  (None)" è¡¨ç¤ºä¸ä½¿ç”¨LoRA
    - lora_weight (float): LoRAæƒé‡ï¼ŒèŒƒå›´0.0-2.0
    - progress: Gradioè¿›åº¦æ¡å¯¹è±¡
    """
    print(f"ğŸ¯ v2ç‰ˆæœ¬å¼€å§‹å¤„ç†: æ¨¡å¼={processing_mode}")
    print(f"ğŸ¨ LoRAè®¾ç½®: æ¨¡å‹={lora_model_name}, æƒé‡={lora_weight}")
    print(f"ğŸŒŠ Optical Flow: {'å¯ç”¨' if OPTICAL_FLOW_AVAILABLE and 'Stable Diffusion' in processing_mode else 'ç¦ç”¨'}")
    
    if input_video_path is None:
        raise gr.Error("è¯·å…ˆä¸Šä¼ ä¸€ä¸ªè§†é¢‘ï¼")

    # 1. å‚æ•°é…ç½®
    config = {
        "output_folder": f"outputs/debug_run_v2_{int(time.time())}",
        "base_model_path": "models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "controlnet_model_path": "models/ControlNet/control_v11p_sd15_lineart.pth",
        "negative_prompt": "low quality, worst quality, blurry, text, logo, watermark, signature",
        "width": 768,
        "height": 512,
        "cfg_scale": 7.5,
        "steps": 20,
        "strength": strength,  
    }

    # 2. å‡†å¤‡å·¥ä½œ
    progress(0, desc="å‡†å¤‡å·¥ä½œï¼šåˆ›å»ºç›®å½•...")
    input_frames_dir, output_frames_dir = setup_directories(config["output_folder"])
    
    progress(0.05, desc="å‡†å¤‡å·¥ä½œï¼šæ‹†åˆ†è§†é¢‘å¸§...")
    fps, frame_total, frames_list = extract_frames(input_video_path, input_frames_dir)  # ã€ä¿®æ”¹ã€‘è·å–å¸§åˆ—è¡¨
    
    # ã€æ–°å¢ã€‘è°ƒæ•´å¸§å°ºå¯¸ç”¨äºoptical flow
    target_size = (config["width"], config["height"])
    frames_resized = []
    for frame in frames_list:
        frame_resized = cv2.resize(frame, target_size)
        frames_resized.append(frame_resized)
    
    # 3. æ ¹æ®æ¨¡å¼ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹
    device = "cuda"
    cyclegan_processor = None
    pipe = None
    preprocessor = None
    
    # åŠ è½½CycleGANæ¨¡å‹
    if "CycleGAN" in processing_mode:
        progress(0.1, desc="åŠ è½½CycleGANæ¨¡å‹...")
        cyclegan_model_name = "own_cyclegan"
        
        cyclegan_netG = 'resnet_9blocks'
        cyclegan_norm = 'instance'
        cyclegan_no_dropout = True

        cyclegan_processor = CycleGANProcessor(
            model_name=cyclegan_model_name,
            netG=cyclegan_netG,
            norm=cyclegan_norm,
            no_dropout=cyclegan_no_dropout,
            gpu_ids='0',
            generator_suffix='_A',
            preserve_resolution=True
        )
        
    # åŠ è½½Stable Diffusionæ¨¡å‹
    if "Stable Diffusion" in processing_mode:
        progress(0.2, desc="åŠ è½½Stable Diffusionå’ŒControlNetæ¨¡å‹...")
        controlnet = ControlNetModel.from_single_file(config["controlnet_model_path"], torch_dtype=torch.float16)
        pipe = StableDiffusionControlNetPipeline.from_single_file(
            config["base_model_path"], controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
        ).to(device)
        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
        preprocessor = LineartAnimeDetector.from_pretrained("lllyasviel/Annotators").to(device)
        
        progress(0.25, desc="åŠ è½½LoRAæ¨¡å‹...")
        lora_success = load_lora_into_pipeline(pipe, lora_model_name, lora_weight)
        if not lora_success:
            print("âš ï¸ LoRAåŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹ç»§ç»­å¤„ç†")

    # 4. æ ¸å¿ƒå¤„ç†å¾ªç¯
    generator = torch.Generator(device=device).manual_seed(int(seed)) if seed != -1 else None
    prev_frame_styled = None  # ã€æ–°å¢ã€‘ç”¨äºoptical flowçš„å‰ä¸€å¸§é£æ ¼åŒ–ç»“æœ

    with torch.no_grad():
        for i, curr_frame in enumerate(progress.tqdm(frames_resized, desc=f"æ­£åœ¨æŒ‰æ¨¡å¼ [{processing_mode}] å¤„ç†æ¯ä¸€å¸§")):
            prev_frame = frames_resized[i-1] if i > 0 else None  # ã€æ–°å¢ã€‘å‰ä¸€å¸§
            result_image = None

            if processing_mode == "CycleGAN Only":
                curr_frame_pil = Image.fromarray(curr_frame)
                stylized_image = cyclegan_processor.process_frame(curr_frame_pil)
                
                if i == 0:
                    try:
                        diag_input_path = os.path.join(config["output_folder"], "z_diagnostic_input.png")
                        diag_output_path = os.path.join(config["output_folder"], "z_diagnostic_output.png")
                        curr_frame_pil.save(diag_input_path)
                        stylized_image.save(diag_output_path)
                        print(f"è¯Šæ–­å›¾åƒå·²ä¿å­˜: {diag_input_path} å’Œ {diag_output_path}")
                        print(f"è¾“å…¥å°ºå¯¸: {curr_frame_pil.size}, è¾“å‡ºå°ºå¯¸: {stylized_image.size}")
                    except Exception as e:
                        print(f"ä¿å­˜è¯Šæ–­å›¾åƒæ—¶å‡ºé”™: {e}")

                result_image = stylized_image
            
            elif processing_mode == "Stable Diffusion Only":
                # ã€æ–°å¢ã€‘ä½¿ç”¨optical flowå¢å¼ºçš„Stable Diffusionå¤„ç†
                result_image = process_frame_with_optical_flow(
                    curr_frame, prev_frame, prev_frame_styled,
                    pipe, preprocessor, prompt, config,
                    device, generator
                )

            elif processing_mode == "CycleGAN + Stable Diffusion":
                curr_frame_pil = Image.fromarray(curr_frame)
                cyclegan_output_image = cyclegan_processor.process_frame(curr_frame_pil)
                cyclegan_output_array = np.array(cyclegan_output_image)
                
                result_image = process_frame_with_optical_flow(
                    cyclegan_output_array, prev_frame, prev_frame_styled,
                    pipe, preprocessor, prompt, config,
                    device, generator
                )
            
            if result_image:
                filename = f"{i:05d}.png"
                result_image.save(os.path.join(output_frames_dir, filename))
                
                # ã€æ–°å¢ã€‘æ›´æ–°prev_frame_styledç”¨äºä¸‹ä¸€å¸§çš„optical flow
                if "Stable Diffusion" in processing_mode:
                    prev_frame_styled = np.array(result_image)

    # ã€æ–°å¢ã€‘æ¸…ç†optical flowæ¨¡å‹å†…å­˜
    if OPTICAL_FLOW_AVAILABLE and "Stable Diffusion" in processing_mode:
        RAFT_clear_memory()

    if pipe and "Stable Diffusion" in processing_mode:
        unload_lora_from_pipeline(pipe)

    # 5. è§†é¢‘åˆæˆ
    progress(0.95, desc="æ­£åœ¨åˆæˆä¸ºæœ€ç»ˆè§†é¢‘...")
    final_video_path = os.path.join(config["output_folder"], "final_video_v2.mp4")
    create_video(output_frames_dir, final_video_path, fps)

    print(f"v2ä»»åŠ¡å®Œæˆï¼è¾“å‡ºè§†é¢‘å·²ä¿å­˜è‡³: {final_video_path}")
    return final_video_path

if __name__ == "__main__":
    print("è¿™æ˜¯v2ç‰ˆæœ¬çš„æ¨¡å—åŒ–è§†é¢‘å¤„ç†è„šæœ¬ï¼Œæ”¯æŒLoRAåŠŸèƒ½å’Œoptical flowç¨³å®šåŒ–ã€‚")
    print("è¯·é€šè¿‡ 'debug_ui_v2.py' æ¥è¿è¡Œå¸¦ç•Œé¢çš„è°ƒè¯•ã€‚") 